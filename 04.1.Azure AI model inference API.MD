# Azure AI model inference API

* The <a href="https://learn.microsoft.com/en-us/azure/ai-studio/reference/reference-model-inference-api?tabs=python">Azure AI model inference API</a> allows developers to deploy and consume AI models via API endpoints without needing extensive infrastructure setup. 
  * It provides a uniform interface for interacting with a variety of AI models, making model deployment and inference straightforward and efficient. Developers can switch between models to compare the performance without changing the underlying code, enhancing flexibility and efficiency. 
  * These models can be deployed to serverless API endpoints (highly scalable and do not require infrastructure management from the user) or managed inference, providing scalability and flexibility for different use cases. 

* Models include traditional AI models and advanced foundation models including small language models (SLMs) like Phi-3 and large language models (LLMs) like GPT-4o.
  * The models available for serverless deployment include - Cohere Embed V3 & Command R, ﻿Meta Llama 2 chat & Llama 3 instruct, Mistral-Small & -Large, Jais, Jamba, ﻿Phi-3

* Main advantages is scalability. The API allows applications to automatically scale based on demand, ensuring that resources are used efficiently and performance remains consistent even during peak usage periods. 
  * Cost-effectiveness is another significant benefit, by utilizing serverless endpoints, only pay for what you use
 
  * Use <a href="https://wandb.ai/">W&B Weave</a> tool to track the inputs and outputs of methods. 

* Azure AI Foundry -> Create Hub (Azure AI Hub) -> Enter resource details ->  From Azure AI Hub -> Launch the Azure AI Studio -> Model Catalog mneu -> Deployment options filter by "serverless inference". 
  * select "gpt-35-turbo-16k" model -> Deploy ->  After deploying the model -> Navigate My assets tab -> select "Models + endpoints" -> Get endpoint URL (Target URL) and the primary key.

* Making inference requests - inference.py
   * Make inference requests to an Azure AI model with integration of Weave for logging, ChatCompletionsClient for handling inference requests and AzureKeyCredential for authentication.
   * get_inference handles the process of sending a prompt to the model. 
      * @weave.op() decorator is used to log the inputs and outputs of the decorated function, allowing you to easily track and analyze the data passing through your models
      * <img src="https://storage.googleapis.com/wandb-production.appspot.com/byyoung3/images/projects/39660617/0d3f9300.png" width=500 height=250>
      * sends a request containing a system message, indicating the assistant's role, and the user's prompt. 
      * captures the response, extracts the content, and prints it to the console.
   ```
    pip install weave
    pip install azure.ai.inference
    
    import weave
    from azure.ai.inference import ChatCompletionsClient
    from azure.ai.inference.models import SystemMessage, UserMessage
    from azure.core.credentials import AzureKeyCredential
    ﻿
    # Initialize Weave for logging
    weave.init('azure-api')
    ﻿
    # Define the endpoint URL and API key (hardcoded for this example)
    ENDPOINT_URL = "your endpoint url"
    PRIMARY_KEY = "your primary key"
    ﻿﻿
    PROMPT = "How many languages are in the world?"
    ﻿﻿
    # Initialize the ChatCompletionsClient
    client = ChatCompletionsClient(
        endpoint=ENDPOINT_URL,
        credential=AzureKeyCredential(PRIMARY_KEY)
    )
    ﻿
    @weave.op
    def run_inference(prompt, client):
        """
        Function to perform inference using the provided client and prompt.
        """
        response = client.complete(
            messages=[
                SystemMessage(content="You are a helpful assistant."),
                UserMessage(content=prompt),
            ]
        )
        try:
            content = response.choices[0].message.content
            print("Response:", content)
            return content
        except Exception as e:
            print(f"Failed to get response: {e}")
            return None
    ﻿
    ﻿
    # Perform inference using the hardcoded prompt
    run_inference(PROMPT, client)
   ```
﻿
* Comparing Multiple Models 
  * One of the great aspects of the Inference API is the consistency of the API across different models, means that you can easily switch to another model or test multiple models with minimal changes to your code. 
  * This is particularly beneficial in scenarios where you need to experiment with different models to find the best fit for your application. 
  * For example, start with a smaller model to quickly test your application's functionality and then switch to a more powerful model. 
  * The consistent API interface allows you to make these transitions smoothly, without the need to rewrite your integration logic.
  ```
   from azure.ai.inference import ChatCompletionsClient
   from azure.ai.inference.models import SystemMessage, UserMessage
   from azure.core.credentials import AzureKeyCredential
   import weave
   ﻿
   # Initialize Weave for logging
   weave.init('azure-api')
   ﻿
   # Define global variables for the two models
   MODEL_1_ENDPOINT_URL = "your first model endpoint url"
   MODEL_1_PRIMARY_KEY = "your first model key"
   ﻿
   MODEL_2_ENDPOINT_URL = "your second model endpoint url"
   MODEL_2_PRIMARY_KEY = "your second model key"
   ﻿
   PROMPT = "How many languages are in the world?"
   ﻿
   # Initialize the ChatCompletionsClient for each model
   client1 = ChatCompletionsClient(
       endpoint=MODEL_1_ENDPOINT_URL,
       credential=AzureKeyCredential(MODEL_1_PRIMARY_KEY)
   )
   ﻿
   client2 = ChatCompletionsClient(
       endpoint=MODEL_2_ENDPOINT_URL,
       credential=AzureKeyCredential(MODEL_2_PRIMARY_KEY)
   )
   ﻿
   ﻿
   @weave.op
   def get_model_prediction(prompt, client):
       response = client.complete(
           messages=[
               SystemMessage(content="You are a helpful assistant."),
               UserMessage(content=prompt),
           ]
       )
       
       try:
           return parse_response(response)
       except Exception as e:
           print(f"Failed to get response: {e}")
           return None
   ﻿
   def parse_response(response):
       if response.choices:
           choice = response.choices[0]
           if choice.message and choice.message.content:
               content = choice.message.content
               print(f"Model response content: {content}")
               return content
           if choice.finish_reason:
               finish_reason = choice.finish_reason
               print(f"Finish reason: {finish_reason}")
       return "No valid response"
   ﻿
   if __name__ == "__main__":
       if not MODEL_1_ENDPOINT_URL or not MODEL_1_PRIMARY_KEY or not MODEL_2_ENDPOINT_URL or not MODEL_2_PRIMARY_KEY:
           print("Error: Missing endpoint URL or primary key for one or both models")
       else:
           print("Response from Model 1:")
           response1 = get_model_prediction(PROMPT, client1)
           if response1:
               print(response1)
           else:
               print("No response or failed to decode the response from Model 1.")
   ﻿
           print("\nResponse from Model 2:")
           response2 = get_model_prediction(PROMPT, client2)
           if response2:
               print(response2)
           else:
               print("No response or failed to decode the response from Model 2.")
   ```


<!--
# https://wandb.ai/byyoung3/ML-NEWS2/reports/A-guide-to-using-the-Azure-AI-model-inference-API--Vmlldzo4OTY1MjEy
-->
