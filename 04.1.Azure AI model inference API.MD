# Azure AI model inference API

* The <a href="https://learn.microsoft.com/en-us/azure/ai-studio/reference/reference-model-inference-api?tabs=python">Azure AI model inference API</a> allows developers to deploy and consume AI models via API endpoints without needing extensive infrastructure setup. 
  * It provides a uniform interface for interacting with a variety of AI models, making model deployment and inference straightforward and efficient. Developers can switch between models to compare the performance without changing the underlying code, enhancing flexibility and efficiency. 
  * These models can be deployed to serverless API endpoints (highly scalable and do not require infrastructure management from the user) or managed inference, providing scalability and flexibility for different use cases. 

* Models include traditional AI models and advanced foundation models including small language models (SLMs) like Phi-3 and large language models (LLMs) like GPT-4o.
  * The models available for serverless deployment include - Cohere Embed V3 & Command R, ﻿Meta Llama 2 chat & Llama 3 instruct, Mistral-Small & -Large, Jais, Jamba, ﻿Phi-3

* Main advantages is scalability. The API allows applications to automatically scale based on demand, ensuring that resources are used efficiently and performance remains consistent even during peak usage periods. 
  * Cost-effectiveness is another significant benefit, by utilizing serverless endpoints, only pay for what you use
 
  * Use <a href="https://wandb.ai/">W&B Weave</a> tool to track the inputs and outputs of methods. 

* Azure AI Foundry -> Create Hub (Azure AI Hub) -> Enter resource details ->  From Azure AI Hub -> Launch the Azure AI Studio -> Model Catalog mneu -> Deployment options filter by "serverless inference". 
  * select "gpt-35-turbo-16k" model -> Deploy ->  After deploying the model -> Navigate My assets tab -> select "Models + endpoints" -> Get endpoint URL (Target URL) and the primary key.

* Code inference.py
   ```
    pip install weave
    pip install azure.ai.inference
    
    import weave
    from azure.ai.inference import ChatCompletionsClient
    from azure.ai.inference.models import SystemMessage, UserMessage
    from azure.core.credentials import AzureKeyCredential
    ﻿
    # Initialize Weave for logging
    weave.init('azure-api')
    ﻿
    # Define the endpoint URL and API key (hardcoded for this example)
    ENDPOINT_URL = "your endpoint url"
    PRIMARY_KEY = "your primary key"
    ﻿﻿
    PROMPT = "How many languages are in the world?"
    ﻿﻿
    # Initialize the ChatCompletionsClient
    client = ChatCompletionsClient(
        endpoint=ENDPOINT_URL,
        credential=AzureKeyCredential(PRIMARY_KEY)
    )
    ﻿
    @weave.op
    def run_inference(prompt, client):
        """
        Function to perform inference using the provided client and prompt.
        """
        response = client.complete(
            messages=[
                SystemMessage(content="You are a helpful assistant."),
                UserMessage(content=prompt),
            ]
        )
        try:
            content = response.choices[0].message.content
            print("Response:", content)
            return content
        except Exception as e:
            print(f"Failed to get response: {e}")
            return None
    ﻿
    ﻿
    # Perform inference using the hardcoded prompt
    run_inference(PROMPT, client)
   ```
﻿
   * Make inference requests to an Azure AI model with integration of Weave for logging, ChatCompletionsClient for handling inference requests and AzureKeyCredential for authentication.
   * Initializes a ChatCompletionsClient using these credentials, allowing it to send requests to the Azure AI service.
   * get_inference handles the process of sending a prompt to the model. 
      * decorated with @weave.op, which enables logging of the operation. 
      * It sends a request containing a system message, indicating the assistant's role, and the user's prompt. 
      * It captures the response, extracts the content, and prints it to the console.

Using Weave to better understand your output
Weave is a powerful tool used for logging inputs and outputs of your model in a central location. The @weave.op() decorator is used to log the inputs and outputs of the decorated function, allowing you to easily track and analyze the data passing through your models. 
The @weave.op decorator is applied to the function that handles the model inference. This decorator logs the function’s inputs and outputs, providing a detailed trace of the function’s execution. By logging this data, Weave helps in debugging and analyzing the model's performance, ensuring that you can trace the flow of data and identify any issues.
Here's what our logs look like inside of Weave:

﻿
﻿
Comparing Multiple Models 
One of the great aspects of the Inference API is the consistency of the API across different models. This consistency means that once you've integrated with one model, you can easily switch to another model or even test multiple models with minimal changes to your code. The API's uniform interface ensures that the same code structure can be reused, which significantly reduces development time and effort.
This is particularly beneficial in scenarios where you need to experiment with different models to find the best fit for your application. For example, you might start with a smaller model to quickly test your application's functionality and then switch to a more powerful model as your requirements evolve. The consistent API interface allows you to make these transitions smoothly, without the need to rewrite your integration logic.
I followed the previous steps in order to create a serverless instance of the Mistral-Small Instruct and a Phi-3 Instruct model, in order to compare the two models on similar prompts. The script provided below is designed to facilitate this comparison process. Additionally, the script integrates logging with Weave. This allows you to maintain a detailed record of the inputs and outputs for each model, making it easier to understand their behavior and performance. Here is the script: 
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential
import weave
﻿
# Initialize Weave for logging
weave.init('azure-api')
﻿
# Define global variables for the two models
MODEL_1_ENDPOINT_URL = "your first model endpoint url"
MODEL_1_PRIMARY_KEY = "your first model key"
﻿
MODEL_2_ENDPOINT_URL = "your second model endpoint url"
MODEL_2_PRIMARY_KEY = "your second model key"
﻿
PROMPT = "How many languages are in the world?"
﻿
# Initialize the ChatCompletionsClient for each model
client1 = ChatCompletionsClient(
    endpoint=MODEL_1_ENDPOINT_URL,
    credential=AzureKeyCredential(MODEL_1_PRIMARY_KEY)
)
﻿
client2 = ChatCompletionsClient(
    endpoint=MODEL_2_ENDPOINT_URL,
    credential=AzureKeyCredential(MODEL_2_PRIMARY_KEY)
)
﻿
﻿
@weave.op
def get_model_prediction(prompt, client):
    response = client.complete(
        messages=[
            SystemMessage(content="You are a helpful assistant."),
            UserMessage(content=prompt),
        ]
    )
    
    try:
        return parse_response(response)
    except Exception as e:
        print(f"Failed to get response: {e}")
        return None
﻿
def parse_response(response):
    if response.choices:
        choice = response.choices[0]
        if choice.message and choice.message.content:
            content = choice.message.content
            print(f"Model response content: {content}")
            return content
        if choice.finish_reason:
            finish_reason = choice.finish_reason
            print(f"Finish reason: {finish_reason}")
    return "No valid response"
﻿
if __name__ == "__main__":
    if not MODEL_1_ENDPOINT_URL or not MODEL_1_PRIMARY_KEY or not MODEL_2_ENDPOINT_URL or not MODEL_2_PRIMARY_KEY:
        print("Error: Missing endpoint URL or primary key for one or both models")
    else:
        print("Response from Model 1:")
        response1 = get_model_prediction(PROMPT, client1)
        if response1:
            print(response1)
        else:
            print("No response or failed to decode the response from Model 1.")
﻿
        print("\nResponse from Model 2:")
        response2 = get_model_prediction(PROMPT, client2)
        if response2:
            print(response2)
        else:
            print("No response or failed to decode the response from Model 2.")
﻿
Inside Weave, we can also display responses based on different inputs and outputs! For this example, we will display only the prompt, endpoint, and output in the Weave trace viewer using a the column manager button on the top right hand side, as shown below: 

﻿

﻿

﻿
﻿
Next, we will add a filter for a specific prompt. In practice, this could be for any specific key passed to the function being logged! We will add a filter for the prompt, as shown below: 

﻿
And after adding this filter, we will see the results down below!

﻿
As we can see, the responses for the Mistral Small Instruct and the Phi-3 Instruct models are quite similar, with the only difference being the format of the integer result, where the Phi-3 model contains a comma in the integer response. Now, this is a very "small" detail, but "small" details mean everything in Machine Learning! Weave allows you to visualize these nuanced responses from your models, and make more informed decisions about choosing the right models and training data for your product!
Evaluating AI Models on a Geography Dataset Using Weave
Now, we’ll demonstrate how to use Weave's evaluation capabilities to compare the performance of two AI models on a custom geography dataset I made using GPT-4o. By leveraging Weave's Evaluation class, we can systematically assess how well each model performs and make informed decisions about which model is better suited for our application.
Introduction to Weave Evaluations
Evaluation-driven development helps you reliably iterate on an application by systematically testing changes against a consistent dataset. The Evaluation class in Weave is designed to assess the performance of a Model on a given dataset using custom scoring functions. This approach ensures that you are comparing results accurately and provides a rich UI to drill into individual outputs and scores, enabling deeper insights into model performance.
The Custom Geography Dataset
The dataset we’ll use consists of geography-related questions paired with their expected answers. Here are a few examples:
[
    {"question": "What is the largest desert in the world?", "expected": "Sahara"},
    {"question": "Which river is the longest in the world?", "expected": "Nile"},
    {"question": "What is the capital of Japan?", "expected": "Tokyo"},
    {"question": "Which ocean is the largest by surface area?", "expected": "Pacific Ocean"},
    {"question": "Which desert is known as the 'Cold Desert'?", "expected": "Gobi Desert"}
]
﻿
This dataset is designed to test the models' ability to generate accurate and relevant responses to common geographical queries.
The Full Script
Below is the Python script that sets up the evaluation, defines the models, and runs the comparison using Weave:
import requests
import json
import time
import asyncio
import weave
from weave import Evaluation, Model
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential
﻿
﻿
# Initialize Weave for logging
weave.init('azure-api-eval-geo')
﻿
﻿
# Define global variables for the two models
MODEL_1_ENDPOINT_URL = "https://Mistral-small-achqr.eastus2.models.ai.azure.com/chat/completions"
MODEL_1_PRIMARY_KEY = "your key"
﻿
MODEL_2_ENDPOINT_URL = "https://Phi-3-small-128k-instruct-fcsez.eastus2.models.ai.azure.com/chat/completions"
MODEL_2_PRIMARY_KEY = "your key"
﻿
﻿
# Initialize the ChatCompletionsClient for each model
def init_client(endpoint, key):
    return ChatCompletionsClient(
        endpoint=endpoint,
        credential=AzureKeyCredential(key)
    )
﻿
﻿
def get_model_prediction(prompt, client, model_name):
    response = client.complete(
        messages=[
            SystemMessage(content="You are a helpful assistant."),
            UserMessage(content=prompt),
        ]
    )
    try:
        content = response.choices[0].message.content
        prompt_tokens = response.usage.prompt_tokens
        completion_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens
        
        # Update the returned result to include the correct token structure
        return {
            'output': content,
            'model': model_name,            
            'usage': {
                'input_tokens': prompt_tokens,
                'output_tokens': completion_tokens,
                'total_tokens' : total_tokens
            }
        }
    except Exception as e:
        print(f"Failed to get response: {e}")
        return {
            'output': "No valid response",
            'usage': {
                'input_tokens': 0,
                'output_tokens': 0,
                'total_tokens': 0
            }
        }
﻿
# Define custom scoring functions
@weave.op()
def substring_match(expected: str, model_output: dict) -> dict:
    match = expected.lower() in model_output['output'].lower()
    return {
        'substring_match': match
    }
﻿
﻿
# Define the base model for Weave
class MistralSmall(Model):
    model_name: str = "Mistral-small"
    endpoint: str = MODEL_1_ENDPOINT_URL
    primary_key: str = MODEL_1_PRIMARY_KEY
    client: ChatCompletionsClient = None  # Declare the client field
﻿
    def __init__(self):
        super().__init__()  # Initialize the base class if needed
        # Initialize the client and assign it explicitly
        object.__setattr__(self, 'client', init_client(self.endpoint, self.primary_key))
﻿
    @weave.op()
    def predict(self, question: str):
        result = get_model_prediction(question, self.client, self.model_name)
        return result
﻿
﻿
class Phi3Small(Model):
    model_name: str = "Phi-3-small"
    endpoint: str = MODEL_2_ENDPOINT_URL
    primary_key: str = MODEL_2_PRIMARY_KEY
    client: ChatCompletionsClient = None  # Declare the client field
﻿
    def __init__(self):
        super().__init__()  # Initialize the base class if needed
        # Initialize the client and assign it explicitly
        object.__setattr__(self, 'client', init_client(self.endpoint, self.primary_key))
    
﻿
    @weave.op()
    def predict(self, question: str):
        result = get_model_prediction(question, self.client, self.model_name)
        return result
﻿
def run_evaluation(model_class):
    # Load the dataset
    with open('dataset.json', 'r') as f:
        dataset = json.load(f)
﻿
    # Instantiate the model
    model = model_class()
﻿
    # Create evaluation for the model
    evaluation = Evaluation(dataset=dataset, scorers=[substring_match])
﻿
    # Run the evaluation for the model
    print(f"Evaluating {model.model_name}:")
    asyncio.run(evaluation.evaluate(model.predict))
    print(f"{model.model_name} evaluation completed.")
﻿
﻿
if __name__ == "__main__":
    # Run evaluation for both models sequentially
    run_evaluation(MistralSmall)
    run_evaluation(Phi3Small)
Explanation of the script
In the initialization and setup phase, Weave is initialized for logging with the name azure-api-eval-geo. The endpoints and primary keys for two models, Mistral-Small and Phi-3-small, are defined. For model prediction, the get_model_prediction function sends a prompt to the specified model endpoint and retrieves the response, using the appropriate API key based on the endpoint provided. 
In addition to retrieving the generated text, the get_model_prediction function also captures token usage statistics from the model output. Specifically, it logs the number of input_tokens (tokens used for the prompt), output_tokens (tokens generated in response), and total_tokens (sum of input and output tokens). This information is structured in the model output and passed to Weave for logging and analysis, enabling easy tracking of resource usage during model evaluations.
A custom scoring function, substring_match, is defined to check if the expected answer is a substring of the generated text from the model. This function is used to evaluate the accuracy of the models' responses. Two model classes, MistralSmall and Phi3Small, are defined, each including a predict method decorated with @weave.op() to log the inputs and outputs during evaluation.
In the evaluation execution phase, the run_evaluation function loads the dataset, instantiates the model class, and runs the evaluation, logging all the data in Weave for further analysis. In the __main__ block, evaluations for both models are run sequentially, allowing for a comparison of their performance on the same dataset.
The Evaluations dashboard
Once the evaluations are complete, Weave provides a rich dashboard to visualize and compare the results. In order to access the evaluations dashboard, simply navigate to the project, and click the evaluations tab in the top left corner (note you will need to select the cells corresponding to your evaluation runs, and you will then be able to select the 'Compare' button at the top right-hand corner of the screen):

﻿
Metrics such as substring match accuracy and model latency can be viewed and compared side by side for each model. This helps in identifying which model is more accurate, faster, or more efficient in terms of token usage. The evaluation dashboard presents a detailed comparison of the models, including accuracy (substring match) and latency, aiding in making data-driven decisions on model selection. 

﻿
﻿
Weave's output comparison feature allows for the specific responses generated by each model for the same input to be compared. For example, when asked "Which desert is known as the 'Cold Desert'?", you can see how each model responds and how well it matches the expected answer. 

﻿
By leveraging Weave’s evaluation tools, a comprehensive understanding of the models' strengths and weaknesses is gained, enabling the selection of the best model for any application.
Conclusion 
The Azure AI Model Inference API provides a robust and flexible solution for deploying and consuming machine learning models via API endpoints. This quick start guide has outlined the key steps to get started, from setting up your Azure environment and deploying models, to making inference requests and leveraging tools like Weave for enhanced logging and analysis.
Additionally, the Weave platform offers powerful evaluation features that enable you to systematically assess your models capabilities. By integrating Weave’s Evaluation class, you can run detailed comparisons between models and quickly visualize key metrics. These features ensure that you can make data-driven decisions when selecting the best model for your specific use case.
By following these steps, developers can efficiently integrate AI capabilities into their applications, benefiting from the scalability, cost-effectiveness, and ease of use provided by Azure's serverless endpoints. The Azure AI Model Inference API simplifies the process, allowing you to focus on building intelligent and responsive applications with pre-trained models.
To maximize your understanding and usage of the Azure AI Model Inference API, it is highly recommended to explore the official Azure documentation. The comprehensive resources available will help you navigate the intricacies of model deployment, API integration, and performance optimization, empowering you to fully harness the potential of Azure AI services in your projects.
